{"cells":[{"cell_type":"markdown","source":["#Evaluation Methods\n","In this notebook, I implemented the necessary methods to evaluate the performance of the models."],"metadata":{"id":"G-k41lFS4vZf"},"id":"G-k41lFS4vZf"},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","path='/content/drive/MyDrive/RecSys_206894495'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D1_zLRtdTud3","executionInfo":{"status":"ok","timestamp":1740151750134,"user_tz":-120,"elapsed":24435,"user":{"displayName":"Netta Mendelbaum","userId":"17523963483759666592"}},"outputId":"48b99eec-e5b5-4b0a-e072-d163d336888a"},"id":"D1_zLRtdTud3","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"id":"041cdfcd","metadata":{"id":"041cdfcd"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import json"]},{"cell_type":"code","source":["def evaluate_model(model, model_csr, user_index, n=10):\n","    user_items = model_csr[user_index]\n","    recommendations = model.recommend(user_index, user_items, N=n)[0]\n","    return recommendations\n","\n","def get_outfit_id_from_index(outfit_indexes, outfit_dict):\n","    return [outfit_dict[idx] for idx in outfit_indexes]"],"metadata":{"id":"jbcPnG2Vpy4R"},"id":"jbcPnG2Vpy4R","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate the mectics at n for a single user\n","\n","#Hit Rate at n - Hit rate at n is the proportion of users for whom the correct item appears in the top n predictions. It is calculated as the number of hits divided by the total number of users.\n","def evaluate_hit_rate_at_n(test_id, predicted_ids, n=10):\n","  if predicted_ids is np.nan:\n","    return 0\n","  if test_id is np.nan:\n","    return 0\n","  if len(predicted_ids) == 0:\n","    return 0\n","  if len(test_id) == 0:\n","    return 0\n","  predicted_ids = predicted_ids[:n]\n","  if type(test_id) == str or type(test_id) == np.str_:\n","    if test_id in predicted_ids:\n","      return 1\n","  elif type(test_id) == list or type(test_id) == np.ndarray:\n","    for outfit_id in test_id:\n","      if outfit_id in predicted_ids:\n","        return 1\n","  else:\n","    raise ValueError(f\"Unknown type {type(test_id)}\")\n","  return 0\n","### Precision at n - Precision at n is the number of relevant items in the top n predictions divided by n.\n","\n","def evaluate_precision_at_n(test_id, predicted_ids, n=10):\n","  if predicted_ids is np.nan:\n","    return 0\n","  if test_id is np.nan:\n","    return 0\n","  if len(predicted_ids) == 0:\n","    return 0\n","  if len(test_id) == 0:\n","    return 0\n","  predicted_ids = predicted_ids[:n]\n","  if type(test_id) == str or type(test_id) == np.str_:\n","    return 1 if test_id in predicted_ids else 0\n","  elif type(test_id) == list or type(test_id) == np.ndarray:\n","    relevant_items = sum(1 for item in test_id if item in predicted_ids)\n","    return relevant_items / n\n","  else:\n","    raise ValueError(f\"Unknown type {type(test_id)}\")\n","\n","### Recall at n - Recall at n is the number of relevant items in the top n predictions divided by the number of relevant items.\n","\n","def evaluate_recall_at_n(test_id, predicted_ids, n=10):\n","  if predicted_ids is np.nan:\n","    return 0\n","  if test_id is np.nan:\n","    return 0\n","  if len(predicted_ids) == 0:\n","    return 0\n","  if len(test_id) == 0:\n","    return 0\n","  predicted_ids = predicted_ids[:n]\n","  if type(test_id) == str or type(test_id) == np.str_:\n","    return 1 if test_id in predicted_ids else 0\n","  elif type(test_id) == list or type(test_id) == np.ndarray:\n","    relevant_items = sum(1 for item in test_id if item in predicted_ids)\n","    if len(test_id) == 0:\n","      return 0\n","    return relevant_items / len(test_id)\n","  else:\n","    raise ValueError(f\"Unknown type {type(test_id)}\")\n","\n","### F1 Score at n - F1 score is the harmonic mean of precision and recall.\n","\n","def evaluate_f1_score_at_n(test_id, predicted_ids, n=10):\n","  if predicted_ids is np.nan:\n","    return 0\n","  if test_id is np.nan:\n","    return 0\n","  if len(predicted_ids) == 0:\n","    return 0\n","  if len(test_id) == 0:\n","    return 0\n","  precision = evaluate_precision_at_n(test_id, predicted_ids, n)\n","  recall = evaluate_recall_at_n(test_id, predicted_ids, n)\n","  if precision + recall == 0:\n","    return 0\n","  return 2 * (precision * recall) / (precision + recall)\n","\n"],"metadata":{"id":"ge9evBBvQlv8"},"id":"ge9evBBvQlv8","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"f7dc25ce","metadata":{"id":"f7dc25ce"},"outputs":[],"source":["# Function to evaluate all metrics and save results to a Parquet file\n","def evaluate_df_metrics_at_n(df, method_name, n=10):\n","    METRIC_COLUMNS = [\"id_hit_rate_at_10\", \"id_precision_at_10\", \"id_recall_at_10\", \"id_f1_score_at_10\",\n","                      \"group_hit_rate_at_10\", \"group_precision_at_10\", \"group_recall_at_10\", \"group_f1_score_at_10\"]\n","    df[\"id_hit_rate_at_10\"] = df.apply(lambda x: evaluate_hit_rate_at_n(x[\"test_outfit_ids\"], x[\"id_prediction\"], n=10), axis=1)\n","    df[\"id_precision_at_10\"] = df.apply(lambda x: evaluate_precision_at_n(x[\"test_outfit_ids\"], x[\"id_prediction\"], n=10), axis=1)\n","    df[\"id_recall_at_10\"] = df.apply(lambda x: evaluate_recall_at_n(x[\"test_outfit_ids\"], x[\"id_prediction\"], n=10), axis=1)\n","    df[\"id_f1_score_at_10\"] = df.apply(lambda x: evaluate_f1_score_at_n(x[\"test_outfit_ids\"], x[\"id_prediction\"], n=10), axis=1)\n","\n","    df[\"group_hit_rate_at_10\"] = df.apply(lambda x: evaluate_hit_rate_at_n(x[\"test_group\"], x[\"group_prediction\"], n=10), axis=1)\n","    df[\"group_precision_at_10\"] = df.apply(lambda x: evaluate_precision_at_n(x[\"test_group\"], x[\"group_prediction\"], n=10), axis=1)\n","    df[\"group_recall_at_10\"] = df.apply(lambda x: evaluate_recall_at_n(x[\"test_group\"], x[\"group_prediction\"], n=10), axis=1)\n","    df[\"group_f1_score_at_10\"] = df.apply(lambda x: evaluate_f1_score_at_n(x[\"test_group\"], x[\"group_prediction\"], n=10), axis=1)\n","\n","    result_dict = {column: df[column].mean() for column in METRIC_COLUMNS}\n","    result_dict['method_name'] = [method_name]\n","    result_df = pd.DataFrame(result_dict)\n","\n","    display(result_df.T)\n","\n","    # Load existing xlsx file if it exists\n","    try:\n","        existing_df = pd.read_excel(path+\"/models/results.xlsx\")\n","        # Check if the method name already exists in the file\n","        if method_name in existing_df['method_name'].values:\n","            # Overwrite the existing row with the new results\n","            existing_df.loc[existing_df['method_name'] == method_name, :] = result_df.iloc[0].values\n","        else:\n","            # Append the new results to the existing file\n","            existing_df = pd.concat([existing_df, result_df], ignore_index=True)\n","\n","        # Save the updated DataFrame back to the Parquet file\n","        existing_df.to_excel(path+\"/models/results.xlsx\",index=False)\n","\n","    except FileNotFoundError:\n","        # If the file does not exist, create a new DataFrame and save it to a Parquet file\n","        result_df.to_excel(path+\"/models/results.xlsx\",index=False)\n","\n","    return df, result_dict"]},{"cell_type":"code","source":["def evaluate_val_metrics_at_n(df, method_name, n=10, model_params=None):\n","    if model_params is None:\n","        model_params = {}\n","\n","    METRIC_COLUMNS = [f\"id_hit_rate_at_{n}\", f\"id_precision_at_{n}\", f\"id_recall_at_{n}\", f\"id_f1_score_at_{n}\",\n","                      f\"group_hit_rate_at_{n}\", f\"group_precision_at_{n}\", f\"group_recall_at_{n}\", f\"group_f1_score_at_{n}\"]\n","\n","    df[f\"id_hit_rate_at_{n}\"] = df.apply(lambda x: evaluate_hit_rate_at_n(x[\"val_outfit_ids\"], x[\"id_prediction\"], n=n), axis=1)\n","    df[f\"id_precision_at_{n}\"] = df.apply(lambda x: evaluate_precision_at_n(x[\"val_outfit_ids\"], x[\"id_prediction\"], n=n), axis=1)\n","    df[f\"id_recall_at_{n}\"] = df.apply(lambda x: evaluate_recall_at_n(x[\"val_outfit_ids\"], x[\"id_prediction\"], n=n), axis=1)\n","    df[f\"id_f1_score_at_{n}\"] = df.apply(lambda x: evaluate_f1_score_at_n(x[\"val_outfit_ids\"], x[\"id_prediction\"], n=n), axis=1)\n","\n","    df[f\"group_hit_rate_at_{n}\"] = df.apply(lambda x: evaluate_hit_rate_at_n(x[\"val_group\"], x[\"group_prediction\"], n=n), axis=1)\n","    df[f\"group_precision_at_{n}\"] = df.apply(lambda x: evaluate_precision_at_n(x[\"val_group\"], x[\"group_prediction\"], n=n), axis=1)\n","    df[f\"group_recall_at_{n}\"] = df.apply(lambda x: evaluate_recall_at_n(x[\"val_group\"], x[\"group_prediction\"], n=n), axis=1)\n","    df[f\"group_f1_score_at_{n}\"] = df.apply(lambda x: evaluate_f1_score_at_n(x[\"val_group\"], x[\"group_prediction\"], n=n), axis=1)\n","\n","    result_dict = {column: df[column].mean() for column in METRIC_COLUMNS}\n","    result_dict['method_name'] = method_name\n","    result_dict['model_params'] = model_params  # Include model parameters in the result\n","    result_df = pd.DataFrame([result_dict])\n","\n","\n","    # Save the result to a file named 'val'\n","    file_path = path + \"/models/val.json\"\n","\n","    # Read the existing JSON file\n","    with open(file_path, 'r') as file:\n","        existing_data = json.load(file)\n","\n","    # Ensure existing_data is a list\n","    if not isinstance(existing_data, list):\n","        existing_data = [existing_data]\n","\n","    # Append new objects to the list\n","    existing_data.append(result_dict)\n","\n","    # Write the updated dictionary back to the file\n","    with open(file_path, 'w') as file:\n","        json.dump(existing_data, file)\n","\n","    return df, result_dict\n"],"metadata":{"id":"RaOucKrmqJ4n"},"id":"RaOucKrmqJ4n","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate_val_metrics_at_n_outfit(df, method_name, n=10, model_params=None):\n","    if model_params is None:\n","        model_params = {}\n","\n","    METRIC_COLUMNS = [f\"id_hit_rate_at_{n}\", f\"id_precision_at_{n}\", f\"id_recall_at_{n}\", f\"id_f1_score_at_{n}\"]\n","\n","    df[f\"id_hit_rate_at_{n}\"] = df.apply(lambda x: evaluate_hit_rate_at_n(x[\"val_outfit_ids\"], x[\"id_prediction\"], n=n), axis=1)\n","    df[f\"id_precision_at_{n}\"] = df.apply(lambda x: evaluate_precision_at_n(x[\"val_outfit_ids\"], x[\"id_prediction\"], n=n), axis=1)\n","    df[f\"id_recall_at_{n}\"] = df.apply(lambda x: evaluate_recall_at_n(x[\"val_outfit_ids\"], x[\"id_prediction\"], n=n), axis=1)\n","    df[f\"id_f1_score_at_{n}\"] = df.apply(lambda x: evaluate_f1_score_at_n(x[\"val_outfit_ids\"], x[\"id_prediction\"], n=n), axis=1)\n","    result_dict = {column: df[column].mean() for column in METRIC_COLUMNS}\n","    result_dict['method_name'] = method_name\n","    result_dict['model_params'] = model_params  # Include model parameters in the result\n","    result_df = pd.DataFrame([result_dict])\n","\n","\n","    # Save the result to a file named 'val'\n","    file_path = path + \"/models/val_outfit.json\"\n","\n","    # Read the existing JSON file\n","    with open(file_path, 'r') as file:\n","        existing_data = json.load(file)\n","\n","    # Ensure existing_data is a list\n","    if not isinstance(existing_data, list):\n","        existing_data = [existing_data]\n","\n","    # Append new objects to the list\n","    existing_data.append(result_dict)\n","\n","    # Write the updated dictionary back to the file\n","    with open(file_path, 'w') as file:\n","        json.dump(existing_data, file)\n","\n","    return df, result_dict\n"],"metadata":{"id":"oklt5CAf93cB"},"id":"oklt5CAf93cB","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate_val_metrics_at_n_group(df, method_name, n=10, model_params=None):\n","    if model_params is None:\n","        model_params = {}\n","\n","    METRIC_COLUMNS = [f\"group_hit_rate_at_{n}\", f\"group_precision_at_{n}\", f\"group_recall_at_{n}\", f\"group_f1_score_at_{n}\"]\n","\n","    df[f\"group_hit_rate_at_{n}\"] = df.apply(lambda x: evaluate_hit_rate_at_n(x[\"val_group\"], x[\"group_prediction\"], n=n), axis=1)\n","    df[f\"group_precision_at_{n}\"] = df.apply(lambda x: evaluate_precision_at_n(x[\"val_group\"], x[\"group_prediction\"], n=n), axis=1)\n","    df[f\"group_recall_at_{n}\"] = df.apply(lambda x: evaluate_recall_at_n(x[\"val_group\"], x[\"group_prediction\"], n=n), axis=1)\n","    df[f\"group_f1_score_at_{n}\"] = df.apply(lambda x: evaluate_f1_score_at_n(x[\"val_group\"], x[\"group_prediction\"], n=n), axis=1)\n","\n","    result_dict = {column: df[column].mean() for column in METRIC_COLUMNS}\n","    result_dict['method_name'] = method_name\n","    result_dict['model_params'] = model_params  # Include model parameters in the result\n","    result_df = pd.DataFrame([result_dict])\n","\n","\n","    # Save the result to a file named 'val'\n","    file_path = path + \"/models/val_group.json\"\n","\n","    # Read the existing JSON file\n","    with open(file_path, 'r') as file:\n","        existing_data = json.load(file)\n","\n","    # Ensure existing_data is a list\n","    if not isinstance(existing_data, list):\n","        existing_data = [existing_data]\n","\n","    # Append new objects to the list\n","    existing_data.append(result_dict)\n","\n","    # Write the updated dictionary back to the file\n","    with open(file_path, 'w') as file:\n","        json.dump(existing_data, file)\n","\n","    return df, result_dict\n"],"metadata":{"id":"dFM_w7QM-aP9"},"id":"dFM_w7QM-aP9","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6060pwZaB4Vf"},"id":"6060pwZaB4Vf","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def retrieve_best_model_params_from_file_outfit( method_name, n):\n","    file_path_outfit = path + \"/models/val_outfit.json\"\n","    with open(file_path_outfit, 'r') as file:\n","        results = json.load(file)\n","    best_hit_rate = -1\n","    for result in results:\n","      if isinstance(result, dict):\n","          if result:\n","            if result['method_name'] == method_name:\n","                if result[f\"id_hit_rate_at_{n}\"] > best_hit_rate:\n","                    best_hit_rate = result[f\"id_hit_rate_at_{n}\"]\n","                    best_params = result['model_params']\n","      else:\n","          print(f\"Unexpected type: {type(result)}\")\n","    return best_params"],"metadata":{"id":"PpnVjeqgBwl6"},"id":"PpnVjeqgBwl6","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def retrieve_best_model_params_from_file_group( method_name, n):\n","    file_path_group = path + \"/models/val_group.json\"\n","    with open(file_path_group, 'r') as file:\n","        results = json.load(file)\n","    best_hit_rate = -1\n","    for result in results:\n","      if isinstance(result, dict):\n","          if result:\n","            if result['method_name'] == method_name:\n","                if result[f\"group_hit_rate_at_{n}\"] > best_hit_rate:\n","                    best_hit_rate = result[f\"group_hit_rate_at_{n}\"]\n","                    best_params = result['model_params']\n","      else:\n","          print(f\"Unexpected type: {type(result)}\")\n","    return best_params"],"metadata":{"id":"sjIs2-h4BzPH"},"id":"sjIs2-h4BzPH","execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"empty_dict = {}\n","with open(path + \"/models/val.json\", 'w') as json_file:\n","    json.dump(empty_dict, json_file)\n","with open(path + \"/models/val_outfit.json\", 'w') as json_file:\n","    json.dump(empty_dict, json_file)\n","with open(path + \"/models/val_group.json\", 'w') as json_file:\n","    json.dump(empty_dict, json_file)\n","\n","print('empty json')\"\"\""],"metadata":{"id":"aTta8CXzTupb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740151752434,"user_tz":-120,"elapsed":1335,"user":{"displayName":"Netta Mendelbaum","userId":"17523963483759666592"}},"outputId":"93b5d036-2778-49bc-f098-5694dae0ee41"},"id":"aTta8CXzTupb","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["empty json\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}