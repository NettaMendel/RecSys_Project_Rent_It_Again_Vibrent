{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d1a9964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in c:\\users\\user\\anaconda3\\lib\\site-packages (18.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4cf3e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "#warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#general\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import product\n",
    "import pickle\n",
    "from sklearn.preprocessing import MultiLabelBinarizer #for Multi Label of the tags\n",
    "\n",
    "#for predictions\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "%run prepare_train_test_splits.ipynb\n",
    "%run evaluate_models.ipynb\n",
    "%run load_embeddings.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f21761e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading embeddings\n",
    "#loaded_embeddings_dict = load_embeddings_from_folder()\n",
    "#pickle.dump(loaded_embeddings_dict, open(EMBEDDING_MODEL_DICT_PICKLE_PATH, \"wb\"))\n",
    "loaded_embeddings_dict = pickle.load(open(EMBEDDING_MODEL_DICT_PICKLE_PATH, \"rb\"))\n",
    "\n",
    "#load data\n",
    "pictures_df = pd.read_parquet('../archive/data/picture_triplets.parquet',engine='pyarrow')\n",
    "orders=pd.read_parquet('../archive/data/orders.parquet',engine='pyarrow')\n",
    "outfits=pd.read_parquet('../archive/data/outfits.parquet',engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f00a0512",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data\n",
    "\n",
    "# Introduce the embeddings for each outfit, if the outfit has no embeddings, we drop it\n",
    "pictures_df[\"embeddings\"] = pictures_df[\"picture.id\"].map(loaded_embeddings_dict)\n",
    "outfit_pictures_df = pictures_df.groupby(\"outfit.id\").agg({\"picture.id\": list, \"embeddings\": list}).reset_index()\n",
    "outfits[\"embeddings\"] = outfits[\"id\"].map(outfit_pictures_df.set_index(\"outfit.id\")[\"embeddings\"])\n",
    "outfits = outfits.dropna(subset=[\"embeddings\"])\n",
    "\n",
    "#convert tag_categories and outfit_tags to lists\n",
    "outfits[\"tag_categories\"] = outfits[\"tag_categories\"].apply(eval)\n",
    "outfits[\"outfit_tags\"] = outfits[\"outfit_tags\"].apply(eval)\n",
    "\n",
    "outfits['group']=outfits['group'].astype(str)\n",
    "\n",
    "# Convert to sets and find common IDs\n",
    "common_ids = set(orders['outfit.id']).intersection(set(outfits['id']))\n",
    "\n",
    "# Filter the dataframes to keep only the common ids\n",
    "orders = orders[orders['outfit.id'].isin(common_ids)]\n",
    "outfits = outfits[outfits['id'].isin(common_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea37477d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare tags\n",
    "all_tags = outfits[\"outfit_tags\"].values.tolist()\n",
    "mlb = MultiLabelBinarizer()\n",
    "one_hot_encoded = mlb.fit_transform(all_tags)\n",
    "outfits[\"one_hot_encoded\"] = [np.array(oh_list) for oh_list in one_hot_encoded.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17fad7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare embeddings\n",
    "def get_mean_embedding(embeddings):\n",
    "    embeddings = np.array(embeddings)\n",
    "    mean_embedding = np.mean(embeddings, axis=0)\n",
    "    return mean_embedding\n",
    "\n",
    "def concatenate_embeddings(oh_embeddings, image_embeddings, oh_weighting):\n",
    "    oh_embeddings = np.array(oh_embeddings) * oh_weighting\n",
    "    return np.concatenate((oh_embeddings, image_embeddings))\n",
    "\n",
    "outfits[\"mean_embeddings\"] = outfits[\"embeddings\"].apply(lambda x: get_mean_embedding(x))\n",
    "#one_hot_encoded = np.array(outfits_df[\"one_hot_encoded\"].tolist())\n",
    "#mean_embeddings = np.array(outfits_df[\"mean_embeddings\"].tolist())\n",
    "\n",
    "outfits[\"concatenated_embeddings\"] = outfits.apply(lambda x: concatenate_embeddings(x[\"one_hot_encoded\"], x[\"mean_embeddings\"], oh_weighting=4), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbf4926a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3598\n",
      "No unique outfit found with groups ['group.423a23f6717e6d85adac54c051ee9832'\n",
      " 'group.423a23f6717e6d85adac54c051ee9832']\n",
      "No unique outfit found with groups ['group.384b8170c6a6ddfd568ff7fab5fb49c4'\n",
      " 'group.384b8170c6a6ddfd568ff7fab5fb49c4']\n",
      "No unique outfit found with groups ['group.a3ab26b5d2f7ef2cf102422a3dde3b46'\n",
      " 'group.a3ab26b5d2f7ef2cf102422a3dde3b46']\n",
      "No unique outfit found with groups ['group.9b5204b87abc93f8f0467b0a6a9c6a97'\n",
      " 'group.9b5204b87abc93f8f0467b0a6a9c6a97'\n",
      " 'group.9b5204b87abc93f8f0467b0a6a9c6a97']\n",
      "No unique outfit found with groups ['group.8e50238120d13b31284f151941c2ee81'\n",
      " 'group.8e50238120d13b31284f151941c2ee81']\n",
      "No unique outfit found with groups ['group.a494d07781a1aab0e3a42989288feff2'\n",
      " 'group.a494d07781a1aab0e3a42989288feff2']\n",
      "No unique outfit found with groups ['group.a1d284ef1c7035dd14e57eba3838a303'\n",
      " 'group.a1d284ef1c7035dd14e57eba3838a303']\n",
      "No unique outfit found with groups ['group.e0cb0f6e113edc4df8a1e304376734f6'\n",
      " 'group.e0cb0f6e113edc4df8a1e304376734f6']\n",
      "No unique outfit found with groups ['group.2c7095c075561fe6278f3a2d7c1d6ac9'\n",
      " 'group.2c7095c075561fe6278f3a2d7c1d6ac9']\n",
      "No unique outfit found with groups ['group.ae8da3f0ad6f8ff3f83b2af96e975991'\n",
      " 'group.ae8da3f0ad6f8ff3f83b2af96e975991']\n",
      "No unique outfit found with groups ['group.4bd4ee24eac8948e82783b15d9404f6b'\n",
      " 'group.4bd4ee24eac8948e82783b15d9404f6b']\n",
      "No unique outfit found with groups ['group.1bfd2412df50ac58b23bd8f52c6b4b35'\n",
      " 'group.1bfd2412df50ac58b23bd8f52c6b4b35']\n",
      "No unique outfit found with groups ['group.edb60c2f440a9ac7d0883fb9371c8607'\n",
      " 'group.edb60c2f440a9ac7d0883fb9371c8607']\n"
     ]
    }
   ],
   "source": [
    "# Convert triplets into entries for each individual user\n",
    "orders = remove_consecutive_duplicates(orders)\n",
    "user_orders_df = translate_user_triplets_to_orders(orders, outfits)\n",
    "user_orders_df.dropna(inplace=True)\n",
    "\n",
    "# Split the data into train and test sets, with one dataframe with no restirictions on outfits in the test data and one that prohibits repeated outfits\n",
    "# It prints any cases in which it is unable to construct a test set with unique outfits.\n",
    "user_splits_df, user_splits_unique_df = convert_user_orders_to_train_test_splits(user_orders_df, percentage_test=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9feead01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['train_outfit_ids', 'test_outfit_id', 'train_group', 'test_group',\n",
       "       'train_booking_times', 'test_booking_time'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_splits_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10034011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_outfit_ids</th>\n",
       "      <th>test_outfit_id</th>\n",
       "      <th>train_group</th>\n",
       "      <th>test_group</th>\n",
       "      <th>train_booking_times</th>\n",
       "      <th>test_booking_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[outfit.85f26909d8334ab78f30c2fc9c73faf7, outf...</td>\n",
       "      <td>[outfit.9f5058295098471abdfaf0a7c74ddbfe]</td>\n",
       "      <td>[group.c79c907b6c94a9bd2005e038943ab529, group...</td>\n",
       "      <td>[group.f6f0b9ebb3228aab27a79ac658c76682]</td>\n",
       "      <td>[2023-11-22 00:00:00, 2023-11-24 00:00:00, 202...</td>\n",
       "      <td>[2023-12-06 00:00:00]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[outfit.d7bff1b799a34575a47ce0f531791c9f, outf...</td>\n",
       "      <td>[outfit.98fa1b5287182a9d, outfit.dd04098010f74...</td>\n",
       "      <td>[group.287dba5268fb7b20e8ef81c053970691, group...</td>\n",
       "      <td>[group.a4449ee16d7951f425083623efd0dcec, group...</td>\n",
       "      <td>[2021-08-02 00:00:00, 2021-08-02 00:00:00, 202...</td>\n",
       "      <td>[2021-11-01 00:00:00, 2021-12-01 00:00:00, 202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[outfit.9fde090f117fb9d9]</td>\n",
       "      <td>[outfit.849ace7e1811150d]</td>\n",
       "      <td>[group.27808d969027a4e243c8945176f280c0]</td>\n",
       "      <td>[group.caafbed55494b0c93dab58d58d526f0a]</td>\n",
       "      <td>[2018-09-06 00:00:00]</td>\n",
       "      <td>[2018-09-06 00:00:00]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[outfit.98eebea274f23dd6, outfit.648db79508724...</td>\n",
       "      <td>[outfit.b2c68e50868a46a8872e81bcd3a17870, outf...</td>\n",
       "      <td>[group.a02de08741b879719c3ea97e24e5f230, group...</td>\n",
       "      <td>[group.69217601bce159dcf21b4c8e6f059f42, group...</td>\n",
       "      <td>[2021-08-25 00:00:00, 2021-08-25 00:00:00, 202...</td>\n",
       "      <td>[2022-02-28 00:00:00, 2022-02-28 00:00:00, 202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[outfit.5e1b9778e36d475699772148e5d4e27b, outf...</td>\n",
       "      <td>[outfit.7321c26a479e46cd9fb07fa3ab7d7594]</td>\n",
       "      <td>[group.0a736bffd33390d7693442e6eecd0f35, group...</td>\n",
       "      <td>[group.cce63b3a8de0f3495c0744990e88b78f]</td>\n",
       "      <td>[2019-11-20 00:00:00, 2019-11-20 00:00:00]</td>\n",
       "      <td>[2019-11-20 00:00:00]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    train_outfit_ids  \\\n",
       "0  [outfit.85f26909d8334ab78f30c2fc9c73faf7, outf...   \n",
       "1  [outfit.d7bff1b799a34575a47ce0f531791c9f, outf...   \n",
       "2                          [outfit.9fde090f117fb9d9]   \n",
       "3  [outfit.98eebea274f23dd6, outfit.648db79508724...   \n",
       "4  [outfit.5e1b9778e36d475699772148e5d4e27b, outf...   \n",
       "\n",
       "                                      test_outfit_id  \\\n",
       "0          [outfit.9f5058295098471abdfaf0a7c74ddbfe]   \n",
       "1  [outfit.98fa1b5287182a9d, outfit.dd04098010f74...   \n",
       "2                          [outfit.849ace7e1811150d]   \n",
       "3  [outfit.b2c68e50868a46a8872e81bcd3a17870, outf...   \n",
       "4          [outfit.7321c26a479e46cd9fb07fa3ab7d7594]   \n",
       "\n",
       "                                         train_group  \\\n",
       "0  [group.c79c907b6c94a9bd2005e038943ab529, group...   \n",
       "1  [group.287dba5268fb7b20e8ef81c053970691, group...   \n",
       "2           [group.27808d969027a4e243c8945176f280c0]   \n",
       "3  [group.a02de08741b879719c3ea97e24e5f230, group...   \n",
       "4  [group.0a736bffd33390d7693442e6eecd0f35, group...   \n",
       "\n",
       "                                          test_group  \\\n",
       "0           [group.f6f0b9ebb3228aab27a79ac658c76682]   \n",
       "1  [group.a4449ee16d7951f425083623efd0dcec, group...   \n",
       "2           [group.caafbed55494b0c93dab58d58d526f0a]   \n",
       "3  [group.69217601bce159dcf21b4c8e6f059f42, group...   \n",
       "4           [group.cce63b3a8de0f3495c0744990e88b78f]   \n",
       "\n",
       "                                 train_booking_times  \\\n",
       "0  [2023-11-22 00:00:00, 2023-11-24 00:00:00, 202...   \n",
       "1  [2021-08-02 00:00:00, 2021-08-02 00:00:00, 202...   \n",
       "2                              [2018-09-06 00:00:00]   \n",
       "3  [2021-08-25 00:00:00, 2021-08-25 00:00:00, 202...   \n",
       "4         [2019-11-20 00:00:00, 2019-11-20 00:00:00]   \n",
       "\n",
       "                                   test_booking_time  \n",
       "0                              [2023-12-06 00:00:00]  \n",
       "1  [2021-11-01 00:00:00, 2021-12-01 00:00:00, 202...  \n",
       "2                              [2018-09-06 00:00:00]  \n",
       "3  [2022-02-28 00:00:00, 2022-02-28 00:00:00, 202...  \n",
       "4                              [2019-11-20 00:00:00]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_splits_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18c33599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NUM_ITEMS = 100\\n\\ndef find_rental_history_embeddings(outfit_ids, outfit_to_embedding_dict):\\n    outfit_ids = [outfit_id for outfit_id in outfit_ids if outfit_id != \"nan\"]\\n    return [outfit_to_embedding_dict[outfit_id] for outfit_id in outfit_ids]\\n\\ndef get_mean_embedding(embeddings):\\n    embeddings = np.array(embeddings)\\n    mean_embedding = np.mean(embeddings, axis=0)\\n    return mean_embedding\\n\\ndef train_gradient_boosting_classifier(embeddings, targets):\\n    model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\\n    model.fit(embeddings, targets)\\n    return model\\n\\ndef predict_with_gradient_boosting_classifier(model, embeddings):\\n    predictions = model.predict(embeddings)\\n    return predictions\\n\\ndef create_target_column(df, column_name):\\n    df[\\'rented\\'] = df[column_name].apply(lambda x: 1 if len(x) > 0 else 0)\\n    return df\\n\\ndef predict_outfit_rental(df, outfits_df, embeddings_column=\"embeddings\"):\\n    df = create_target_column(df, \\'train_outfit_ids\\')\\n    \\n    outfits_df.dropna(subset=embeddings_column, inplace=True)\\n    outfit_to_embedding_dict = outfits_df.set_index(\"id\")[embeddings_column].to_dict()\\n    \\n    df[\"train_id_embeddings\"] = df[\"train_outfit_ids\"].apply(lambda x: find_rental_history_embeddings(x, outfit_to_embedding_dict))\\n    df[\"rental_history_id_embedding\"] = df[\"train_id_embeddings\"].apply(lambda x: get_mean_embedding(x))\\n    \\n    embeddings = np.stack(outfits_df[embeddings_column].values)\\n    targets = df[\"rented\"].values\\n\\n    id_embeddings = np.stack(df[\"rental_history_id_embedding\"].values)\\n\\n    id_model = train_gradient_boosting_classifier(embeddings, targets)\\n    id_predictions = predict_with_gradient_boosting_classifier(id_model, id_embeddings)\\n\\n    df[\"id_prediction\"] = id_predictions\\n    \\n    return df\\n\\ndef predict_group_rental(df, outfits_df, embeddings_column=\"embeddings\"):\\n    df = create_target_column(df, \\'train_group\\')\\n    \\n    outfits_df.dropna(subset=embeddings_column, inplace=True)\\n    group_to_embedding_dict = outfits_df.set_index(\"group\")[embeddings_column].to_dict()\\n    \\n    df[\"train_group_embeddings\"] = df[\"train_group\"].apply(lambda x: find_rental_history_embeddings(x, group_to_embedding_dict))\\n    df[\"rental_history_group_embedding\"] = df[\"train_group_embeddings\"].apply(lambda x: get_mean_embedding(x))\\n    \\n    embeddings = np.stack(outfits_df[embeddings_column].values)\\n    targets = df[\"rented\"].values\\n\\n    group_embeddings = np.stack(df[\"rental_history_group_embedding\"].values)\\n\\n    group_model = train_gradient_boosting_classifier(embeddings, targets)\\n    group_predictions = predict_with_gradient_boosting_classifier(group_model, group_embeddings)\\n\\n    df[\"group_prediction\"] = group_predictions\\n    \\n    return df\\n\\ndef predict_nearest_neighbors(df, outfits_df, embeddings_column=\"embeddings\"):\\n    df = predict_outfit_rental(df, outfits_df, embeddings_column=embeddings_column)\\n    df = predict_group_rental(df, outfits_df, embeddings_column=embeddings_column)\\n    return df\\ndef predict_nearest_neighbors_images(df, outfits_df, embeddings_column=\"embeddings\"):\\n    outfits_df[\"mean_embeddings\"] = outfits_df[embeddings_column].apply(lambda x: get_mean_embedding(x))\\n    df = predict_outfit_rental(df, outfits_df, embeddings_column=\"mean_embeddings\")\\n    df = predict_group_rental(df, outfits_df, embeddings_column=\"mean_embeddings\")\\n    return df\\n\\n\\n# Apply to dataframes\\ntqdm.pandas()'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"NUM_ITEMS = 100\n",
    "\n",
    "def find_rental_history_embeddings(outfit_ids, outfit_to_embedding_dict):\n",
    "    outfit_ids = [outfit_id for outfit_id in outfit_ids if outfit_id != \"nan\"]\n",
    "    return [outfit_to_embedding_dict[outfit_id] for outfit_id in outfit_ids]\n",
    "\n",
    "def get_mean_embedding(embeddings):\n",
    "    embeddings = np.array(embeddings)\n",
    "    mean_embedding = np.mean(embeddings, axis=0)\n",
    "    return mean_embedding\n",
    "\n",
    "def train_gradient_boosting_classifier(embeddings, targets):\n",
    "    model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "    model.fit(embeddings, targets)\n",
    "    return model\n",
    "\n",
    "def predict_with_gradient_boosting_classifier(model, embeddings):\n",
    "    predictions = model.predict(embeddings)\n",
    "    return predictions\n",
    "\n",
    "def create_target_column(df, column_name):\n",
    "    df['rented'] = df[column_name].apply(lambda x: 1 if len(x) > 0 else 0)\n",
    "    return df\n",
    "\n",
    "def predict_outfit_rental(df, outfits_df, embeddings_column=\"embeddings\"):\n",
    "    df = create_target_column(df, 'train_outfit_ids')\n",
    "    \n",
    "    outfits_df.dropna(subset=embeddings_column, inplace=True)\n",
    "    outfit_to_embedding_dict = outfits_df.set_index(\"id\")[embeddings_column].to_dict()\n",
    "    \n",
    "    df[\"train_id_embeddings\"] = df[\"train_outfit_ids\"].apply(lambda x: find_rental_history_embeddings(x, outfit_to_embedding_dict))\n",
    "    df[\"rental_history_id_embedding\"] = df[\"train_id_embeddings\"].apply(lambda x: get_mean_embedding(x))\n",
    "    \n",
    "    embeddings = np.stack(outfits_df[embeddings_column].values)\n",
    "    targets = df[\"rented\"].values\n",
    "\n",
    "    id_embeddings = np.stack(df[\"rental_history_id_embedding\"].values)\n",
    "\n",
    "    id_model = train_gradient_boosting_classifier(embeddings, targets)\n",
    "    id_predictions = predict_with_gradient_boosting_classifier(id_model, id_embeddings)\n",
    "\n",
    "    df[\"id_prediction\"] = id_predictions\n",
    "    \n",
    "    return df\n",
    "\n",
    "def predict_group_rental(df, outfits_df, embeddings_column=\"embeddings\"):\n",
    "    df = create_target_column(df, 'train_group')\n",
    "    \n",
    "    outfits_df.dropna(subset=embeddings_column, inplace=True)\n",
    "    group_to_embedding_dict = outfits_df.set_index(\"group\")[embeddings_column].to_dict()\n",
    "    \n",
    "    df[\"train_group_embeddings\"] = df[\"train_group\"].apply(lambda x: find_rental_history_embeddings(x, group_to_embedding_dict))\n",
    "    df[\"rental_history_group_embedding\"] = df[\"train_group_embeddings\"].apply(lambda x: get_mean_embedding(x))\n",
    "    \n",
    "    embeddings = np.stack(outfits_df[embeddings_column].values)\n",
    "    targets = df[\"rented\"].values\n",
    "\n",
    "    group_embeddings = np.stack(df[\"rental_history_group_embedding\"].values)\n",
    "\n",
    "    group_model = train_gradient_boosting_classifier(embeddings, targets)\n",
    "    group_predictions = predict_with_gradient_boosting_classifier(group_model, group_embeddings)\n",
    "\n",
    "    df[\"group_prediction\"] = group_predictions\n",
    "    \n",
    "    return df\n",
    "\n",
    "def predict_nearest_neighbors(df, outfits_df, embeddings_column=\"embeddings\"):\n",
    "    df = predict_outfit_rental(df, outfits_df, embeddings_column=embeddings_column)\n",
    "    df = predict_group_rental(df, outfits_df, embeddings_column=embeddings_column)\n",
    "    return df\n",
    "def predict_nearest_neighbors_images(df, outfits_df, embeddings_column=\"embeddings\"):\n",
    "    outfits_df[\"mean_embeddings\"] = outfits_df[embeddings_column].apply(lambda x: get_mean_embedding(x))\n",
    "    df = predict_outfit_rental(df, outfits_df, embeddings_column=\"mean_embeddings\")\n",
    "    df = predict_group_rental(df, outfits_df, embeddings_column=\"mean_embeddings\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply to dataframes\n",
    "tqdm.pandas()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45e286a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def find_rental_history_embeddings(outfit_ids, outfit_to_embedding_dict):\n",
    "    outfit_ids = [outfit_id for outfit_id in outfit_ids if outfit_id != \"nan\"]\n",
    "    return [outfit_to_embedding_dict[outfit_id] for outfit_id in outfit_ids]\n",
    "\n",
    "def get_mean_embedding(embeddings):\n",
    "    embeddings = np.array(embeddings)\n",
    "    mean_embedding = np.mean(embeddings, axis=0)\n",
    "    return mean_embedding\n",
    "\n",
    "def prepare_data(df, outfits_df, embeddings_column=\"embeddings\"):\n",
    "    outfits_df.dropna(subset=[embeddings_column], inplace=True)\n",
    "    outfit_to_embedding_dict = outfits_df.set_index(\"id\")[embeddings_column].to_dict()\n",
    "    group_to_embedding_dict = outfits_df.set_index(\"group\")[embeddings_column].to_dict()\n",
    "    \n",
    "    df[\"train_id_embeddings\"] = df[\"train_outfit_ids\"].apply(lambda x: find_rental_history_embeddings(x, outfit_to_embedding_dict))\n",
    "    df[\"train_group_embeddings\"] = df[\"train_group\"].apply(lambda x: find_rental_history_embeddings(x, group_to_embedding_dict))\n",
    "\n",
    "    df[\"rental_history_id_embedding\"] = df[\"train_id_embeddings\"].apply(lambda x: get_mean_embedding(x))\n",
    "    df[\"rental_history_group_embedding\"] = df[\"train_group_embeddings\"].apply(lambda x: get_mean_embedding(x))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def generate_negative_samples(df, outfits_df, embeddings_column=\"embeddings\"):\n",
    "    all_outfit_ids = set(outfits_df[\"id\"].values)\n",
    "    all_group_ids = set(outfits_df[\"group\"].values)\n",
    "    \n",
    "    df[\"negative_outfit_ids\"] = df[\"train_outfit_ids\"].apply(lambda x: list(all_outfit_ids - set(x)))\n",
    "    df[\"negative_group_ids\"] = df[\"train_group\"].apply(lambda x: list(all_group_ids - set(x)))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_balanced_dataset(df, embeddings_column=\"embeddings\", sample_size=10000):\n",
    "    positive_samples = df[[\"rental_history_id_embedding\", \"test_outfit_id\"]].copy()\n",
    "    positive_samples[\"label\"] = 1\n",
    "    \n",
    "    negative_samples = df[[\"rental_history_id_embedding\", \"negative_outfit_ids\"]].explode(\"negative_outfit_ids\").copy()\n",
    "    negative_samples.rename(columns={\"negative_outfit_ids\": \"test_outfit_id\"}, inplace=True)\n",
    "    negative_samples[\"label\"] = 0\n",
    "    \n",
    "    balanced_df = pd.concat([positive_samples, negative_samples], ignore_index=True)\n",
    "    \n",
    "    # Sample a subset of the data\n",
    "    balanced_df = balanced_df.sample(n=sample_size, random_state=42)\n",
    "    \n",
    "    return balanced_df\n",
    "\n",
    "def train_gradient_boosting(df, outfits_df, embeddings_column=\"embeddings\", sample_size=10000):\n",
    "    df = prepare_data(df, outfits_df, embeddings_column)\n",
    "    df = generate_negative_samples(df, outfits_df, embeddings_column)\n",
    "    \n",
    "    balanced_df = create_balanced_dataset(df, embeddings_column, sample_size)\n",
    "    \n",
    "    X_outfit = np.stack(balanced_df[\"rental_history_id_embedding\"].values)\n",
    "    y_outfit = balanced_df[\"label\"].values\n",
    "    \n",
    "    model_outfit = GradientBoostingClassifier()\n",
    "    model_outfit.fit(X_outfit, y_outfit)\n",
    "    \n",
    "    return model_outfit\n",
    "\n",
    "def predict_with_gradient_boosting(df, model_outfit, outfits_df, embeddings_column=\"embeddings\"):\n",
    "    df = prepare_data(df, outfits_df, embeddings_column)\n",
    "    \n",
    "    X_outfit = np.stack(df[\"rental_history_id_embedding\"].values)\n",
    "    \n",
    "    outfit_predictions = model_outfit.predict(X_outfit)\n",
    "    \n",
    "    df[\"id_prediction\"] = outfit_predictions\n",
    "    return df\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "197d639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ITEMS = 100\n",
    "\n",
    "def find_rental_history_embeddings(outfit_ids, outfit_to_embedding_dict):\n",
    "    outfit_ids = [outfit_id for outfit_id in outfit_ids if outfit_id != \"nan\"]\n",
    "    return [outfit_to_embedding_dict[outfit_id] for outfit_id in outfit_ids]\n",
    "\n",
    "def get_mean_embedding(embeddings):\n",
    "    embeddings = np.array(embeddings)\n",
    "    mean_embedding = np.mean(embeddings, axis=0)\n",
    "    return mean_embedding\n",
    "\n",
    "def prepare_data(df, outfits_df, embeddings_column=\"embeddings\"):\n",
    "    outfits_df.dropna(subset=[embeddings_column], inplace=True)\n",
    "    outfit_to_embedding_dict = outfits_df.set_index(\"id\")[embeddings_column].to_dict()\n",
    "    group_to_embedding_dict = outfits_df.set_index(\"group\")[embeddings_column].to_dict()\n",
    "    \n",
    "    df[\"train_id_embeddings\"] = df[\"train_outfit_ids\"].apply(lambda x: find_rental_history_embeddings(x, outfit_to_embedding_dict))\n",
    "    df[\"train_group_embeddings\"] = df[\"train_group\"].apply(lambda x: find_rental_history_embeddings(x, group_to_embedding_dict))\n",
    "\n",
    "    df[\"rental_history_id_embedding\"] = df[\"train_id_embeddings\"].apply(lambda x: get_mean_embedding(x))\n",
    "    df[\"rental_history_group_embedding\"] = df[\"train_group_embeddings\"].apply(lambda x: get_mean_embedding(x))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def generate_negative_samples(df, outfits_df, embeddings_column=\"embeddings\"):\n",
    "    all_outfit_ids = set(outfits_df[\"id\"].values)\n",
    "    all_group_ids = set(outfits_df[\"group\"].values)\n",
    "    \n",
    "    df[\"negative_outfit_ids\"] = df[\"train_outfit_ids\"].apply(lambda x: list(all_outfit_ids - set(x)))\n",
    "    df[\"negative_group_ids\"] = df[\"train_group\"].apply(lambda x: list(all_group_ids - set(x)))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_balanced_dataset(df, embeddings_column=\"embeddings\"):\n",
    "    positive_samples = df[[\"rental_history_id_embedding\", \"test_outfit_id\"]].copy()\n",
    "    positive_samples[\"label\"] = 1\n",
    "    \n",
    "    # Ensure negative samples are generated\n",
    "    if \"negative_outfit_ids\" not in df.columns:\n",
    "        df = generate_negative_samples(df, outfits_df, embeddings_column)\n",
    "    \n",
    "    negative_samples = df[[\"rental_history_id_embedding\", \"negative_outfit_ids\"]].explode(\"negative_outfit_ids\").copy()\n",
    "    negative_samples.rename(columns={\"negative_outfit_ids\": \"test_outfit_id\"}, inplace=True)\n",
    "    negative_samples[\"label\"] = 0\n",
    "    \n",
    "    # Balance the dataset by resampling\n",
    "    n_positive = len(positive_samples)\n",
    "    negative_samples = negative_samples.sample(n=n_positive, random_state=42)\n",
    "    \n",
    "    balanced_df = pd.concat([positive_samples, negative_samples], ignore_index=True)\n",
    "    \n",
    "    return balanced_df\n",
    "\n",
    "def train_gradient_boosting(df, outfits_df, embeddings_column=\"embeddings\", sample_size=10000):\n",
    "    df = prepare_data(df, outfits_df, embeddings_column)\n",
    "    df = generate_negative_samples(df, outfits_df, embeddings_column)\n",
    "    \n",
    "    balanced_df = create_balanced_dataset(df, embeddings_column)\n",
    "    \n",
    "    X_outfit = np.stack(balanced_df[\"rental_history_id_embedding\"].values)\n",
    "    y_outfit = balanced_df[\"label\"].values\n",
    "    \n",
    "    model_outfit = GradientBoostingClassifier()\n",
    "    model_outfit.fit(X_outfit, y_outfit)\n",
    "    \n",
    "    return model_outfit\n",
    "\n",
    "def predict_with_gradient_boosting(df, model_outfit, outfits_df, embeddings_column=\"embeddings\"):\n",
    "    df = prepare_data(df, outfits_df, embeddings_column)\n",
    "    \n",
    "    X_outfit = np.stack(df[\"rental_history_id_embedding\"].values)\n",
    "    \n",
    "    outfit_predictions = model_outfit.predict_proba(X_outfit)[:, 1]  # Get probabilities for the positive class\n",
    "    df[\"predicted_probabilities\"] = outfit_predictions\n",
    "    \n",
    "    # Get top N recommendations\n",
    "    df[\"id_prediction\"] = df[\"predicted_probabilities\"].apply(lambda x: np.argsort(x)[-100:][::-1].tolist())\n",
    "    df[\"group_prediction\"] = df[\"predicted_probabilities\"].apply(lambda x: np.argsort(x)[-100:][::-1].tolist())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4bb4dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to dataframes\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "58e535ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_rental_history_embeddings(outfit_ids, outfit_to_embedding_dict):\n",
    "    outfit_ids = [outfit_id for outfit_id in outfit_ids if outfit_id != \"nan\"]\n",
    "    return [outfit_to_embedding_dict[outfit_id] for outfit_id in outfit_ids]\n",
    "\n",
    "def get_mean_embedding(embeddings):\n",
    "    embeddings = np.array(embeddings)\n",
    "    mean_embedding = np.mean(embeddings, axis=0)\n",
    "    return mean_embedding\n",
    "\n",
    "def prepare_data(df, outfits_df, embeddings_column=\"embeddings\"):\n",
    "    outfits_df.dropna(subset=[embeddings_column], inplace=True)\n",
    "    outfit_to_embedding_dict = outfits_df.set_index(\"id\")[embeddings_column].to_dict()\n",
    "    group_to_embedding_dict = outfits_df.set_index(\"group\")[embeddings_column].to_dict()\n",
    "    \n",
    "    df[\"train_id_embeddings\"] = df[\"train_outfit_ids\"].apply(lambda x: find_rental_history_embeddings(x, outfit_to_embedding_dict))\n",
    "    df[\"train_group_embeddings\"] = df[\"train_group\"].apply(lambda x: find_rental_history_embeddings(x, group_to_embedding_dict))\n",
    "\n",
    "    df[\"rental_history_id_embedding\"] = df[\"train_id_embeddings\"].apply(lambda x: get_mean_embedding(x))\n",
    "    df[\"rental_history_group_embedding\"] = df[\"train_group_embeddings\"].apply(lambda x: get_mean_embedding(x))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def generate_negative_samples(df, outfits_df, embeddings_column=\"embeddings\"):\n",
    "    all_outfit_ids = set(outfits_df[\"id\"].values)\n",
    "    all_group_ids = set(outfits_df[\"group\"].values)\n",
    "    \n",
    "    df[\"negative_outfit_ids\"] = df[\"train_outfit_ids\"].apply(lambda x: list(all_outfit_ids - set(x)))\n",
    "    df[\"negative_group_ids\"] = df[\"train_group\"].apply(lambda x: list(all_group_ids - set(x)))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_balanced_dataset(df,outfits_df, embeddings_column=\"embeddings\"):\n",
    "    positive_samples = df[[\"rental_history_id_embedding\", \"test_outfit_id\"]].copy()\n",
    "    positive_samples[\"label\"] = 1\n",
    "    \n",
    "    # Ensure negative samples are generated\n",
    "    if \"negative_outfit_ids\" not in df.columns:\n",
    "        df = generate_negative_samples(df, outfits_df, embeddings_column)\n",
    "    \n",
    "    negative_samples = df[[\"rental_history_id_embedding\", \"negative_outfit_ids\"]].explode(\"negative_outfit_ids\").copy()\n",
    "    negative_samples.rename(columns={\"negative_outfit_ids\": \"test_outfit_id\"}, inplace=True)\n",
    "    negative_samples[\"label\"] = 0\n",
    "    \n",
    "    # Balance the dataset by resampling\n",
    "    n_positive = len(positive_samples)\n",
    "    negative_samples = negative_samples.sample(n=n_positive, random_state=42)\n",
    "    \n",
    "    balanced_df = pd.concat([positive_samples, negative_samples], ignore_index=True)\n",
    "    \n",
    "    return balanced_df\n",
    "\n",
    "def train_gradient_boosting(df, outfits_df, embeddings_column=\"embeddings\"):\n",
    "    df = prepare_data(df, outfits_df, embeddings_column)\n",
    "    \n",
    "    balanced_df = create_balanced_dataset(df,outfits_df, embeddings_column)\n",
    "    \n",
    "    X_outfit = np.stack(balanced_df[\"rental_history_id_embedding\"].values)\n",
    "    y_outfit = balanced_df[\"label\"].values\n",
    "    \n",
    "    model_outfit = GradientBoostingClassifier()\n",
    "    model_outfit.fit(X_outfit, y_outfit)\n",
    "    \n",
    "    return model_outfit\n",
    "\n",
    "def predict_with_gradient_boosting(df, model_outfit, outfits_df, embeddings_column=\"embeddings\"):\n",
    "    df = prepare_data(df, outfits_df, embeddings_column)\n",
    "    \n",
    "    X_outfit = np.stack(df[\"rental_history_id_embedding\"].values)\n",
    "    \n",
    "    outfit_predictions = model_outfit.predict_proba(X_outfit)[:, 1]  # Get probabilities for the positive class\n",
    "    df[\"predicted_probabilities\"] = outfit_predictions\n",
    "    \n",
    "    # Map indices to outfit IDs\n",
    "    outfit_id_map = {i: outfit_id for i, outfit_id in enumerate(outfits_df[\"id\"].values)}\n",
    "    \n",
    "    # Get top N recommendations\n",
    "    df[\"id_prediction\"] = df[\"predicted_probabilities\"].apply(lambda x: [outfit_id_map[i] for i in np.argsort(x)[-100:][::-1]])\n",
    "    df[\"group_prediction\"] = df[\"predicted_probabilities\"].apply(lambda x: [outfit_id_map[i] for i in np.argsort(x)[-100:][::-1]])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "da8bc8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_rental_history_embeddings(outfit_ids, outfit_to_embedding_dict):\n",
    "    outfit_ids = [outfit_id for outfit_id in outfit_ids if outfit_id != \"nan\"]\n",
    "    return [outfit_to_embedding_dict[outfit_id] for outfit_id in outfit_ids]\n",
    "\n",
    "def get_mean_embedding(embeddings):\n",
    "    embeddings = np.array(embeddings)\n",
    "    mean_embedding = np.mean(embeddings, axis=0)\n",
    "    return mean_embedding\n",
    "\n",
    "def prepare_data(df, outfits_df, embeddings_column=\"embeddings\"):\n",
    "    outfits_df.dropna(subset=[embeddings_column], inplace=True)\n",
    "    outfit_to_embedding_dict = outfits_df.set_index(\"id\")[embeddings_column].to_dict()\n",
    "    group_to_embedding_dict = outfits_df.set_index(\"group\")[embeddings_column].to_dict()\n",
    "    \n",
    "    df[\"train_id_embeddings\"] = df[\"train_outfit_ids\"].apply(lambda x: find_rental_history_embeddings(x, outfit_to_embedding_dict))\n",
    "    df[\"train_group_embeddings\"] = df[\"train_group\"].apply(lambda x: find_rental_history_embeddings(x, group_to_embedding_dict))\n",
    "\n",
    "    df[\"rental_history_id_embedding\"] = df[\"train_id_embeddings\"].apply(lambda x: get_mean_embedding(x))\n",
    "    df[\"rental_history_group_embedding\"] = df[\"train_group_embeddings\"].apply(lambda x: get_mean_embedding(x))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def generate_negative_samples(df, outfits_df, embeddings_column=\"embeddings\"):\n",
    "    all_outfit_ids = set(outfits_df[\"id\"].values)\n",
    "    all_group_ids = set(outfits_df[\"group\"].values)\n",
    "    \n",
    "    df[\"negative_outfit_ids\"] = df[\"train_outfit_ids\"].apply(lambda x: list(all_outfit_ids - set(x)))\n",
    "    df[\"negative_group_ids\"] = df[\"train_group\"].apply(lambda x: list(all_group_ids - set(x)))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_balanced_dataset(df,outfits_df, embeddings_column=\"embeddings\"):\n",
    "    positive_samples = df[[\"rental_history_id_embedding\", \"test_outfit_id\"]].copy()\n",
    "    positive_samples[\"label\"] = 1\n",
    "    \n",
    "    # Ensure negative samples are generated\n",
    "    if \"negative_outfit_ids\" not in df.columns:\n",
    "        df = generate_negative_samples(df, outfits_df, embeddings_column)\n",
    "    \n",
    "    negative_samples = df[[\"rental_history_id_embedding\", \"negative_outfit_ids\"]].explode(\"negative_outfit_ids\").copy()\n",
    "    negative_samples.rename(columns={\"negative_outfit_ids\": \"test_outfit_id\"}, inplace=True)\n",
    "    negative_samples[\"label\"] = 0\n",
    "    \n",
    "    # Balance the dataset by resampling\n",
    "    n_positive = len(positive_samples)\n",
    "    negative_samples = negative_samples.sample(n=n_positive, random_state=42)\n",
    "    \n",
    "    balanced_df = pd.concat([positive_samples, negative_samples], ignore_index=True)\n",
    "    \n",
    "    return balanced_df\n",
    "\n",
    "def train_gradient_boosting(df, outfits_df, embeddings_column=\"embeddings\"):\n",
    "    df = prepare_data(df, outfits_df, embeddings_column)\n",
    "    \n",
    "    balanced_df = create_balanced_dataset(df,outfits_df, embeddings_column)\n",
    "    \n",
    "    X_outfit = np.stack(balanced_df[\"rental_history_id_embedding\"].values)\n",
    "    y_outfit = balanced_df[\"label\"].values\n",
    "    \n",
    "    model_outfit = GradientBoostingClassifier()\n",
    "    model_outfit.fit(X_outfit, y_outfit)\n",
    "    \n",
    "    return model_outfit\n",
    "\n",
    "def predict_with_gradient_boosting(df, model_outfit, outfits_df, embeddings_column=\"embeddings\"):\n",
    "    df = prepare_data(df, outfits_df, embeddings_column)\n",
    "    \n",
    "    X_outfit = np.stack(df[\"rental_history_id_embedding\"].values)\n",
    "    \n",
    "    outfit_predictions = model_outfit.predict_proba(X_outfit)[:, 1]  # Get probabilities for the positive class\n",
    "    df[\"predicted_probabilities\"] = outfit_predictions\n",
    "    \n",
    "    # Map indices to outfit IDs\n",
    "    outfit_id_map = {i: outfit_id for i, outfit_id in enumerate(outfits_df[\"id\"].values)}\n",
    "    \n",
    "    # Get top N recommendations for each user\n",
    "    df[\"id_prediction\"] = df.apply(lambda row: [outfit_id_map[i] for i in np.argsort(row[\"predicted_probabilities\"])[-100:][::-1]], axis=1)\n",
    "    df[\"group_prediction\"] = df.apply(lambda row: [outfit_id_map[i] for i in np.argsort(row[\"predicted_probabilities\"])[-100:][::-1]], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41643deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_images(df, outfits_df, embeddings_column=\"embeddings\"):\n",
    "    outfits_df[\"mean_embeddings\"] = outfits_df[embeddings_column].apply(lambda x: get_mean_embedding(x))\n",
    "    return train_gradient_boosting(df, outfits_df, embeddings_column=\"mean_embeddings\")\n",
    "    \n",
    "def predict_nearest_neighbors_images(df, outfits_df, embeddings_column=\"embeddings\"):\n",
    "    outfits_df[\"mean_embeddings\"] = outfits_df[embeddings_column].apply(lambda x: get_mean_embedding(x))\n",
    "    return predict_with_gradient_boosting(df, outfits_df, embeddings_column=\"mean_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bc778e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_outfit_ids</th>\n",
       "      <th>test_outfit_id</th>\n",
       "      <th>train_group</th>\n",
       "      <th>test_group</th>\n",
       "      <th>train_booking_times</th>\n",
       "      <th>test_booking_time</th>\n",
       "      <th>train_id_embeddings</th>\n",
       "      <th>train_group_embeddings</th>\n",
       "      <th>rental_history_id_embedding</th>\n",
       "      <th>rental_history_group_embedding</th>\n",
       "      <th>negative_outfit_ids</th>\n",
       "      <th>negative_group_ids</th>\n",
       "      <th>predicted_probabilities</th>\n",
       "      <th>id_prediction</th>\n",
       "      <th>group_prediction</th>\n",
       "      <th>id_hit_rate_at_100</th>\n",
       "      <th>id_hit_rate_at_10</th>\n",
       "      <th>group_hit_rate_at_100</th>\n",
       "      <th>group_hit_rate_at_10</th>\n",
       "      <th>contains_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [train_outfit_ids, test_outfit_id, train_group, test_group, train_booking_times, test_booking_time, train_id_embeddings, train_group_embeddings, rental_history_id_embedding, rental_history_group_embedding, negative_outfit_ids, negative_group_ids, predicted_probabilities, id_prediction, group_prediction, id_hit_rate_at_100, id_hit_rate_at_10, group_hit_rate_at_100, group_hit_rate_at_10, contains_value]\n",
       "Index: []"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df['contains_value'] = predictions_df['id_prediction'].apply(lambda x: 'outfit.fffdaa715c3646f8b1c0f04d549ff07e' in x)\n",
    "predictions_df[predictions_df['contains_value']!=True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d89eb27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d9416674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_outfit_ids</th>\n",
       "      <th>test_outfit_id</th>\n",
       "      <th>train_group</th>\n",
       "      <th>test_group</th>\n",
       "      <th>train_booking_times</th>\n",
       "      <th>test_booking_time</th>\n",
       "      <th>train_id_embeddings</th>\n",
       "      <th>train_group_embeddings</th>\n",
       "      <th>rental_history_id_embedding</th>\n",
       "      <th>rental_history_group_embedding</th>\n",
       "      <th>negative_outfit_ids</th>\n",
       "      <th>negative_group_ids</th>\n",
       "      <th>predicted_probabilities</th>\n",
       "      <th>id_prediction</th>\n",
       "      <th>group_prediction</th>\n",
       "      <th>id_hit_rate_at_100</th>\n",
       "      <th>id_hit_rate_at_10</th>\n",
       "      <th>group_hit_rate_at_100</th>\n",
       "      <th>group_hit_rate_at_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[outfit.85f26909d8334ab78f30c2fc9c73faf7, outf...</td>\n",
       "      <td>[outfit.9f5058295098471abdfaf0a7c74ddbfe]</td>\n",
       "      <td>[group.c79c907b6c94a9bd2005e038943ab529, group...</td>\n",
       "      <td>[group.f6f0b9ebb3228aab27a79ac658c76682]</td>\n",
       "      <td>[2023-11-22 00:00:00, 2023-11-24 00:00:00, 202...</td>\n",
       "      <td>[2023-12-06 00:00:00]</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[outfit.7815f82c055c4ecebb71e3ba2dbb764f, outf...</td>\n",
       "      <td>[group.178ab9526dc35eaa207170039c1381d2, group...</td>\n",
       "      <td>0.553572</td>\n",
       "      <td>[outfit.fffdaa715c3646f8b1c0f04d549ff07e]</td>\n",
       "      <td>[outfit.fffdaa715c3646f8b1c0f04d549ff07e]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[outfit.d7bff1b799a34575a47ce0f531791c9f, outf...</td>\n",
       "      <td>[outfit.98fa1b5287182a9d, outfit.dd04098010f74...</td>\n",
       "      <td>[group.287dba5268fb7b20e8ef81c053970691, group...</td>\n",
       "      <td>[group.a4449ee16d7951f425083623efd0dcec, group...</td>\n",
       "      <td>[2021-08-02 00:00:00, 2021-08-02 00:00:00, 202...</td>\n",
       "      <td>[2021-11-01 00:00:00, 2021-12-01 00:00:00, 202...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[outfit.7815f82c055c4ecebb71e3ba2dbb764f, outf...</td>\n",
       "      <td>[group.178ab9526dc35eaa207170039c1381d2, group...</td>\n",
       "      <td>0.518006</td>\n",
       "      <td>[outfit.fffdaa715c3646f8b1c0f04d549ff07e]</td>\n",
       "      <td>[outfit.fffdaa715c3646f8b1c0f04d549ff07e]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[outfit.9fde090f117fb9d9]</td>\n",
       "      <td>[outfit.849ace7e1811150d]</td>\n",
       "      <td>[group.27808d969027a4e243c8945176f280c0]</td>\n",
       "      <td>[group.caafbed55494b0c93dab58d58d526f0a]</td>\n",
       "      <td>[2018-09-06 00:00:00]</td>\n",
       "      <td>[2018-09-06 00:00:00]</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[outfit.7815f82c055c4ecebb71e3ba2dbb764f, outf...</td>\n",
       "      <td>[group.178ab9526dc35eaa207170039c1381d2, group...</td>\n",
       "      <td>0.499688</td>\n",
       "      <td>[outfit.fffdaa715c3646f8b1c0f04d549ff07e]</td>\n",
       "      <td>[outfit.fffdaa715c3646f8b1c0f04d549ff07e]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[outfit.98eebea274f23dd6, outfit.648db79508724...</td>\n",
       "      <td>[outfit.b2c68e50868a46a8872e81bcd3a17870, outf...</td>\n",
       "      <td>[group.a02de08741b879719c3ea97e24e5f230, group...</td>\n",
       "      <td>[group.69217601bce159dcf21b4c8e6f059f42, group...</td>\n",
       "      <td>[2021-08-25 00:00:00, 2021-08-25 00:00:00, 202...</td>\n",
       "      <td>[2022-02-28 00:00:00, 2022-02-28 00:00:00, 202...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[outfit.7815f82c055c4ecebb71e3ba2dbb764f, outf...</td>\n",
       "      <td>[group.178ab9526dc35eaa207170039c1381d2, group...</td>\n",
       "      <td>0.450340</td>\n",
       "      <td>[outfit.fffdaa715c3646f8b1c0f04d549ff07e]</td>\n",
       "      <td>[outfit.fffdaa715c3646f8b1c0f04d549ff07e]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[outfit.5e1b9778e36d475699772148e5d4e27b, outf...</td>\n",
       "      <td>[outfit.7321c26a479e46cd9fb07fa3ab7d7594]</td>\n",
       "      <td>[group.0a736bffd33390d7693442e6eecd0f35, group...</td>\n",
       "      <td>[group.cce63b3a8de0f3495c0744990e88b78f]</td>\n",
       "      <td>[2019-11-20 00:00:00, 2019-11-20 00:00:00]</td>\n",
       "      <td>[2019-11-20 00:00:00]</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[outfit.7815f82c055c4ecebb71e3ba2dbb764f, outf...</td>\n",
       "      <td>[group.178ab9526dc35eaa207170039c1381d2, group...</td>\n",
       "      <td>0.501527</td>\n",
       "      <td>[outfit.fffdaa715c3646f8b1c0f04d549ff07e]</td>\n",
       "      <td>[outfit.fffdaa715c3646f8b1c0f04d549ff07e]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    train_outfit_ids  \\\n",
       "0  [outfit.85f26909d8334ab78f30c2fc9c73faf7, outf...   \n",
       "1  [outfit.d7bff1b799a34575a47ce0f531791c9f, outf...   \n",
       "2                          [outfit.9fde090f117fb9d9]   \n",
       "3  [outfit.98eebea274f23dd6, outfit.648db79508724...   \n",
       "4  [outfit.5e1b9778e36d475699772148e5d4e27b, outf...   \n",
       "\n",
       "                                      test_outfit_id  \\\n",
       "0          [outfit.9f5058295098471abdfaf0a7c74ddbfe]   \n",
       "1  [outfit.98fa1b5287182a9d, outfit.dd04098010f74...   \n",
       "2                          [outfit.849ace7e1811150d]   \n",
       "3  [outfit.b2c68e50868a46a8872e81bcd3a17870, outf...   \n",
       "4          [outfit.7321c26a479e46cd9fb07fa3ab7d7594]   \n",
       "\n",
       "                                         train_group  \\\n",
       "0  [group.c79c907b6c94a9bd2005e038943ab529, group...   \n",
       "1  [group.287dba5268fb7b20e8ef81c053970691, group...   \n",
       "2           [group.27808d969027a4e243c8945176f280c0]   \n",
       "3  [group.a02de08741b879719c3ea97e24e5f230, group...   \n",
       "4  [group.0a736bffd33390d7693442e6eecd0f35, group...   \n",
       "\n",
       "                                          test_group  \\\n",
       "0           [group.f6f0b9ebb3228aab27a79ac658c76682]   \n",
       "1  [group.a4449ee16d7951f425083623efd0dcec, group...   \n",
       "2           [group.caafbed55494b0c93dab58d58d526f0a]   \n",
       "3  [group.69217601bce159dcf21b4c8e6f059f42, group...   \n",
       "4           [group.cce63b3a8de0f3495c0744990e88b78f]   \n",
       "\n",
       "                                 train_booking_times  \\\n",
       "0  [2023-11-22 00:00:00, 2023-11-24 00:00:00, 202...   \n",
       "1  [2021-08-02 00:00:00, 2021-08-02 00:00:00, 202...   \n",
       "2                              [2018-09-06 00:00:00]   \n",
       "3  [2021-08-25 00:00:00, 2021-08-25 00:00:00, 202...   \n",
       "4         [2019-11-20 00:00:00, 2019-11-20 00:00:00]   \n",
       "\n",
       "                                   test_booking_time  \\\n",
       "0                              [2023-12-06 00:00:00]   \n",
       "1  [2021-11-01 00:00:00, 2021-12-01 00:00:00, 202...   \n",
       "2                              [2018-09-06 00:00:00]   \n",
       "3  [2022-02-28 00:00:00, 2022-02-28 00:00:00, 202...   \n",
       "4                              [2019-11-20 00:00:00]   \n",
       "\n",
       "                                 train_id_embeddings  \\\n",
       "0  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "1  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "2  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "3  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "4  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "\n",
       "                              train_group_embeddings  \\\n",
       "0  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "1  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "2  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "3  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "4  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "\n",
       "                         rental_history_id_embedding  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                      rental_history_group_embedding  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                 negative_outfit_ids  \\\n",
       "0  [outfit.7815f82c055c4ecebb71e3ba2dbb764f, outf...   \n",
       "1  [outfit.7815f82c055c4ecebb71e3ba2dbb764f, outf...   \n",
       "2  [outfit.7815f82c055c4ecebb71e3ba2dbb764f, outf...   \n",
       "3  [outfit.7815f82c055c4ecebb71e3ba2dbb764f, outf...   \n",
       "4  [outfit.7815f82c055c4ecebb71e3ba2dbb764f, outf...   \n",
       "\n",
       "                                  negative_group_ids  predicted_probabilities  \\\n",
       "0  [group.178ab9526dc35eaa207170039c1381d2, group...                 0.553572   \n",
       "1  [group.178ab9526dc35eaa207170039c1381d2, group...                 0.518006   \n",
       "2  [group.178ab9526dc35eaa207170039c1381d2, group...                 0.499688   \n",
       "3  [group.178ab9526dc35eaa207170039c1381d2, group...                 0.450340   \n",
       "4  [group.178ab9526dc35eaa207170039c1381d2, group...                 0.501527   \n",
       "\n",
       "                               id_prediction  \\\n",
       "0  [outfit.fffdaa715c3646f8b1c0f04d549ff07e]   \n",
       "1  [outfit.fffdaa715c3646f8b1c0f04d549ff07e]   \n",
       "2  [outfit.fffdaa715c3646f8b1c0f04d549ff07e]   \n",
       "3  [outfit.fffdaa715c3646f8b1c0f04d549ff07e]   \n",
       "4  [outfit.fffdaa715c3646f8b1c0f04d549ff07e]   \n",
       "\n",
       "                            group_prediction  id_hit_rate_at_100  \\\n",
       "0  [outfit.fffdaa715c3646f8b1c0f04d549ff07e]                   0   \n",
       "1  [outfit.fffdaa715c3646f8b1c0f04d549ff07e]                   0   \n",
       "2  [outfit.fffdaa715c3646f8b1c0f04d549ff07e]                   0   \n",
       "3  [outfit.fffdaa715c3646f8b1c0f04d549ff07e]                   0   \n",
       "4  [outfit.fffdaa715c3646f8b1c0f04d549ff07e]                   0   \n",
       "\n",
       "   id_hit_rate_at_10  group_hit_rate_at_100  group_hit_rate_at_10  \n",
       "0                  0                      0                     0  \n",
       "1                  0                      0                     0  \n",
       "2                  0                      0                     0  \n",
       "3                  0                      0                     0  \n",
       "4                  0                      0                     0  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "56343ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id_hit_rate_at_100       0.0\n",
       "id_hit_rate_at_10        0.0\n",
       "group_hit_rate_at_100    0.0\n",
       "group_hit_rate_at_10     0.0\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tag based predictions\n",
    "user_splits=user_splits_df.copy()\n",
    "#user_splits_unique=user_splits_unique_df.copy()\n",
    "model_outfit = train_gradient_boosting(user_splits, outfits ,embeddings_column=\"one_hot_encoded\")\n",
    "predictions_df = predict_with_gradient_boosting(user_splits, model_outfit, outfits,embeddings_column=\"one_hot_encoded\")\n",
    "#user_splits_unique = predict_nearest_neighbors(user_splits_unique, outfits, embeddings_column=\"one_hot_encoded\")\n",
    "final_df, hit_rate_results = evaluate_df_hit_rate_at_n(predictions_df, n=10)\n",
    "#user_splits_unique = evaluate_df_hit_rate_at_n(user_splits_unique, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fbbe2832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_outfit_ids</th>\n",
       "      <th>test_outfit_id</th>\n",
       "      <th>train_group</th>\n",
       "      <th>test_group</th>\n",
       "      <th>train_booking_times</th>\n",
       "      <th>test_booking_time</th>\n",
       "      <th>train_id_embeddings</th>\n",
       "      <th>train_group_embeddings</th>\n",
       "      <th>rental_history_id_embedding</th>\n",
       "      <th>rental_history_group_embedding</th>\n",
       "      <th>negative_outfit_ids</th>\n",
       "      <th>negative_group_ids</th>\n",
       "      <th>predicted_probabilities</th>\n",
       "      <th>id_prediction</th>\n",
       "      <th>group_prediction</th>\n",
       "      <th>id_hit_rate_at_100</th>\n",
       "      <th>id_hit_rate_at_10</th>\n",
       "      <th>group_hit_rate_at_100</th>\n",
       "      <th>group_hit_rate_at_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [train_outfit_ids, test_outfit_id, train_group, test_group, train_booking_times, test_booking_time, train_id_embeddings, train_group_embeddings, rental_history_id_embedding, rental_history_group_embedding, negative_outfit_ids, negative_group_ids, predicted_probabilities, id_prediction, group_prediction, id_hit_rate_at_100, id_hit_rate_at_10, group_hit_rate_at_100, group_hit_rate_at_10]\n",
       "Index: []"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df[final_df['id_hit_rate_at_100']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2de195b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image based predictions\n",
    "user_splits=user_splits_df.copy()\n",
    "user_splits_unique=user_splits_unique_df.copy()\n",
    "user_splits = predict_nearest_neighbors_images(user_splits, outfits, embeddings_column=\"embeddings\")\n",
    "user_splits_unique = predict_nearest_neighbors_images(user_splits_unique, outfits, embeddings_column=\"embeddings\")\n",
    "user_splits = evaluate_df_hit_rate_at_n(user_splits, n=10)\n",
    "user_splits_unique = evaluate_df_hit_rate_at_n(user_splits_unique, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32499c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined predictions\n",
    "\"\"\"\n",
    "user_splits=user_splits_df.copy()\n",
    "user_splits_unique=user_splits_unique_df.copy()\n",
    "user_splits = predict_nearest_neighbors(user_splits, outfits, embeddings_column=\"outfit_embeddings\")\n",
    "user_splits_unique = predict_nearest_neighbors(user_splits_unique, outfits, embeddings_column=\"outfit_embeddings\")\n",
    "user_splits = evaluate_df_hit_rate_at_n(user_splits, n=10)\n",
    "user_splits_unique = evaluate_df_hit_rate_at_n(user_splits_unique, n=10)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2547466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat predictions\n",
    "user_splits=user_splits_df.copy()\n",
    "user_splits_unique=user_splits_unique_df.copy()\n",
    "user_splits = predict_nearest_neighbors(user_splits, outfits, embeddings_column=\"concatenated_embeddings\")\n",
    "user_splits_unique = predict_nearest_neighbors(user_splits_unique, outfits, embeddings_column=\"concatenated_embeddings\")\n",
    "user_splits = evaluate_df_hit_rate_at_n(user_splits, n=10)\n",
    "user_splits_unique = evaluate_df_hit_rate_at_n(user_splits_unique, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a436d21d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
